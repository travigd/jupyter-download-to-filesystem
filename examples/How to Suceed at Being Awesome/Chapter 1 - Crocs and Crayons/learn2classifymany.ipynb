{
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "jbook": {
   "sections": [
    {
     "name": "learnmany"
    }
   ],
   "jbook_id": "5a63b006a3e4ef776541dd17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
      "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization-cells----run-this-first!\" data-toc-modified-id=\"Initialization-cells----run-this-first!-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization cells -- run this first!</a></span></li><li><span><a href=\"#Learning-to-fit--scalar-valued-functions-with-a-neural-network\" data-toc-modified-id=\"Learning-to-fit--scalar-valued-functions-with-a-neural-network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning to fit  scalar valued functions with a neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modeling-the-input-output-relation\" data-toc-modified-id=\"Modeling-the-input-output-relation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Modeling the input-output relation</a></span></li><li><span><a href=\"#The-mean-squared-error-loss-function-and-its-gradient\" data-toc-modified-id=\"The-mean-squared-error-loss-function-and-its-gradient-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The mean-squared error loss function and its gradient</a></span></li><li><span><a href=\"#Using-the-network-to-learn-a-univariate,-scalar-valued-function\" data-toc-modified-id=\"Using-the-network-to-learn-a-univariate,-scalar-valued-function-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Using the network to learn a univariate, scalar valued function</a></span></li><li><span><a href=\"#Understanding-how-a-neural-network-tells-apart-data-from-two-categories\" data-toc-modified-id=\"Understanding-how-a-neural-network-tells-apart-data-from-two-categories-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Understanding how a neural network tells apart data from two categories</a></span></li><li><span><a href=\"#Linear-separability-of-two-classes\" data-toc-modified-id=\"Linear-separability-of-two-classes-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Linear separability of two classes</a></span></li><li><span><a href=\"#Separating-linearly-non-separable-classes-with-a-neural-network\" data-toc-modified-id=\"Separating-linearly-non-separable-classes-with-a-neural-network-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Separating linearly non-separable classes with a neural network</a></span></li></ul></li><li><span><a href=\"#Fitting-vector-valued-multivariate-functions\" data-toc-modified-id=\"Fitting-vector-valued-multivariate-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fitting vector valued multivariate functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Application:-Handwriting-recognition----more-than-2-digits-at-a-time\" data-toc-modified-id=\"Application:-Handwriting-recognition----more-than-2-digits-at-a-time-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Application: Handwriting recognition -- more than 2 digits at a time</a></span></li><li><span><a href=\"#Recognizing-your-handwriting\" data-toc-modified-id=\"Recognizing-your-handwriting-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Recognizing <em>your</em> handwriting</a></span></li></ul></li><li><span><a href=\"#Summary-and-where-to-next\" data-toc-modified-id=\"Summary-and-where-to-next-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Summary and where to next</a></span></li></ul></div>"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "toc": true,
    "heading_collapsed": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization-cells----run-this-first!\" data-toc-modified-id=\"Initialization-cells----run-this-first!-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization cells -- run this first!</a></span></li><li><span><a href=\"#Learning-to-fit--scalar-valued-functions-with-a-neural-network\" data-toc-modified-id=\"Learning-to-fit--scalar-valued-functions-with-a-neural-network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning to fit  scalar valued functions with a neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modeling-the-input-output-relation\" data-toc-modified-id=\"Modeling-the-input-output-relation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Modeling the input-output relation</a></span></li><li><span><a href=\"#The-mean-squared-error-loss-function-and-its-gradient\" data-toc-modified-id=\"The-mean-squared-error-loss-function-and-its-gradient-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The mean-squared error loss function and its gradient</a></span></li><li><span><a href=\"#Using-the-network-to-learn-a-univariate,-scalar-valued-function\" data-toc-modified-id=\"Using-the-network-to-learn-a-univariate,-scalar-valued-function-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Using the network to learn a univariate, scalar valued function</a></span></li><li><span><a href=\"#Understanding-how-a-neural-network-tells-apart-data-from-two-categories\" data-toc-modified-id=\"Understanding-how-a-neural-network-tells-apart-data-from-two-categories-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Understanding how a neural network tells apart data from two categories</a></span></li><li><span><a href=\"#Linear-separability-of-two-classes\" data-toc-modified-id=\"Linear-separability-of-two-classes-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Linear separability of two classes</a></span></li><li><span><a href=\"#Separating-linearly-non-separable-classes-with-a-neural-network\" data-toc-modified-id=\"Separating-linearly-non-separable-classes-with-a-neural-network-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Separating linearly non-separable classes with a neural network</a></span></li></ul></li><li><span><a href=\"#Fitting-vector-valued-multivariate-functions\" data-toc-modified-id=\"Fitting-vector-valued-multivariate-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fitting vector valued multivariate functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Application:-Handwriting-recognition----more-than-2-digits-at-a-time\" data-toc-modified-id=\"Application:-Handwriting-recognition----more-than-2-digits-at-a-time-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Application: Handwriting recognition -- more than 2 digits at a time</a></span></li><li><span><a href=\"#Recognizing-your-handwriting\" data-toc-modified-id=\"Recognizing-your-handwriting-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Recognizing <em>your</em> handwriting</a></span></li></ul></li><li><span><a href=\"#Summary-and-where-to-next\" data-toc-modified-id=\"Summary-and-where-to-next-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Summary and where to next</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "# Initialization cells -- run this first!"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization cells -- run this first!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "CodeCell",
     "editable": true,
     "original_source": [
      "using Plots, Interact\n",
      "gr()"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "using Plots, Interact\n",
    "gr()"
   ],
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "# Learning to fit  scalar valued functions with a neural network"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning to fit  scalar valued functions with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "We saw in the previous chapters how neural networks could be used to learn a function a scalar valued function (Chapter 1, where we fit $f(x)$ ) and a multivariate function (in Chapter 2, where we trained a network to distinguish between two digits).\n",
      "\n",
      "Our goal in this chapter is to get a deeper understanding and appreciation of the power of non-linear activation functions. By end of this chapter you will know how to train a more general network than we have encountered so far -- this network will contain an artbirary number of input neurons and an arbitrary number of  neurons in a so-called hidden layer  and with either one or an equal number of neurons in the output layer as the hidden layer.\n",
      "\n",
      "\n",
      "We will  use this network to:\n",
      "\n",
      "- learn univariate and multivariate scalar valued functions including the univariate functions  in Chapter 1\n",
      "\n",
      "- revisit how to discriminate between two classes and to demonstrate what a `tanh` network that classifies a challenging dataset is able to data that a `linear` network is unable to\n",
      "\n",
      "- learn how to tell apart more than two classes \n",
      "\n",
      "- learn recognize all 10 digits (0-9) in your handwriting!\n",
      "\n",
      "Let us begin!"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw in the previous chapters how neural networks could be used to learn a function a scalar valued function (Chapter 1, where we fit $f(x)$ ) and a multivariate function (in Chapter 2, where we trained a network to distinguish between two digits).\n",
    "\n",
    "Our goal in this chapter is to get a deeper understanding and appreciation of the power of non-linear activation functions. By end of this chapter you will know how to train a more general network than we have encountered so far -- this network will contain an artbirary number of input neurons and an arbitrary number of  neurons in a so-called hidden layer  and with either one or an equal number of neurons in the output layer as the hidden layer.\n",
    "\n",
    "\n",
    "We will  use this network to:\n",
    "\n",
    "- learn univariate and multivariate scalar valued functions including the univariate functions  in Chapter 1\n",
    "\n",
    "- revisit how to discriminate between two classes and to demonstrate what a `tanh` network that classifies a challenging dataset is able to data that a `linear` network is unable to\n",
    "\n",
    "- learn how to tell apart more than two classes \n",
    "\n",
    "- learn recognize all 10 digits (0-9) in your handwriting!\n",
    "\n",
    "Let us begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "## Modeling the input-output relation"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling the input-output relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "We begin with a network that can (we hope) learn a multivariate scalar valued function."
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We begin with a network that can (we hope) learn a multivariate scalar valued function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "<img src=\"singleclass_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
      "\n",
      "The  network we will consider has $d$ inputs (or input neurons), one hidden layer consisting of $n$ neurons and an output layer consisting of a single output neuron.  corresponding to the inputs $x_1, \\ldots, x_n$. The figure above shows such a configuration for $d = 4$ and $n = 5$. \n",
      "\n",
      "Let $w_{ij}$ denote the weight of the edge from the input neuron $j$ to the neuron $i$ in the hidden layer and let $b_1, \n",
      "\\ldots,b_n$ denote the respective biases.\n",
      "\n",
      "\n",
      "The output of the first neuron in the hidden layer is given by\n",
      "\n",
      "\\begin{equation}\n",
      "g_{\\color{blue}{1}}(x) = f_{\\rm a}(w_{\\color{blue}{1}1} x_1 + w_{\\color{blue}{1} 2} x_2 + \\ldots + w_{\\color{blue}{1} d} x_d + b_{\\color{blue}{1}}).\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Similarly, the output of the second neuron in the hidden layer is given by\n",
      "\n",
      "\\begin{equation}\n",
      "g_{\\color{blue}{2}}(x) = f_{\\rm a}(w_{\\color{blue}{2}1} x_1 + w_{\\color{blue}{2} 2} x_2 + \\ldots + w_{\\color{blue}{2} d} x_d + b_{\\color{blue}{2}}).\n",
      "\\end{equation}\n",
      "\n",
      "More generally if we define $x$ to be the vector of inputs\n",
      "\n",
      "$$ x = \\begin{bmatrix} x_1 & \\ldots & x_{d} \\end{bmatrix}^{T},$$\n",
      "\n",
      "then\n",
      "\n",
      "\\begin{equation}\n",
      "g_{\\color{blue}{i}}(x) = f_{\\rm a}(w_{\\color{blue}{i}1} x_1 + w_{\\color{blue}{i} 2} x_2 + \\ldots + w_{\\color{blue}{i} d} x_d + b_{\\color{blue}{i}}).\n",
      "\\end{equation}\n",
      "\n",
      "The output of the neural network is given by\n",
      "\n",
      "\\begin{equation}\n",
      "g(x) = \\sum_{i=1}^n g_i(x)\n",
      "\\end{equation}\n",
      "\n",
      "which is equivalent to the expression  \n",
      "\n",
      "\\begin{equation}\n",
      "g(x) = \\sum_{i=1}^{n} f_{\\rm a}\\left(\\sum_{j=1}^{d} w_{ij} x_j + b_i\\right),\n",
      "\\end{equation}\n",
      "where $f_{\\rm a}(z)$ is the activation function.\n"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"singleclass_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The  network we will consider has $d$ inputs (or input neurons), one hidden layer consisting of $n$ neurons and an output layer consisting of a single output neuron.  corresponding to the inputs $x_1, \\ldots, x_n$. The figure above shows such a configuration for $d = 4$ and $n = 5$. \n",
    "\n",
    "Let $w_{ij}$ denote the weight of the edge from the input neuron $j$ to the neuron $i$ in the hidden layer and let $b_1, \n",
    "\\ldots,b_n$ denote the respective biases.\n",
    "\n",
    "\n",
    "The output of the first neuron in the hidden layer is given by\n",
    "\n",
    "\\begin{equation}\n",
    "g_{\\color{blue}{1}}(x) = f_{\\rm a}(w_{\\color{blue}{1}1} x_1 + w_{\\color{blue}{1} 2} x_2 + \\ldots + w_{\\color{blue}{1} d} x_d + b_{\\color{blue}{1}}).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Similarly, the output of the second neuron in the hidden layer is given by\n",
    "\n",
    "\\begin{equation}\n",
    "g_{\\color{blue}{2}}(x) = f_{\\rm a}(w_{\\color{blue}{2}1} x_1 + w_{\\color{blue}{2} 2} x_2 + \\ldots + w_{\\color{blue}{2} d} x_d + b_{\\color{blue}{2}}).\n",
    "\\end{equation}\n",
    "\n",
    "More generally if we define $x$ to be the vector of inputs\n",
    "\n",
    "$$ x = \\begin{bmatrix} x_1 & \\ldots & x_{d} \\end{bmatrix}^{T},$$\n",
    "\n",
    "then\n",
    "\n",
    "\\begin{equation}\n",
    "g_{\\color{blue}{i}}(x) = f_{\\rm a}(w_{\\color{blue}{i}1} x_1 + w_{\\color{blue}{i} 2} x_2 + \\ldots + w_{\\color{blue}{i} d} x_d + b_{\\color{blue}{i}}).\n",
    "\\end{equation}\n",
    "\n",
    "The output of the neural network is given by\n",
    "\n",
    "\\begin{equation}\n",
    "g(x) = \\sum_{i=1}^n g_i(x)\n",
    "\\end{equation}\n",
    "\n",
    "which is equivalent to the expression  \n",
    "\n",
    "\\begin{equation}\n",
    "g(x) = \\sum_{i=1}^{n} f_{\\rm a}\\left(\\sum_{j=1}^{d} w_{ij} x_j + b_i\\right),\n",
    "\\end{equation}\n",
    "where $f_{\\rm a}(z)$ is the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "TextCell",
     "editable": true,
     "original_source": [
      "Let us rewrite this expression via a matrix vector product and the \"dot\" operator.\n",
      "\n",
      "Define the $n \\times d$ matrix\n",
      "\n",
      "$$W = \\begin{bmatrix} w_{11} & \\ldots & w_{1d}  \\\\\n",
      "                      w_{21} & \\ldots & w_{2d} \\\\\n",
      "                      \\vdots & \\vdots & \\vdots \\\\\n",
      "                      w_{n1} & \\ldots & w_{nd} \\end{bmatrix}.$$\n",
      "                      \n",
      "Then, for the vector of biases\n",
      "\n",
      "$$ b = \\begin{bmatrix} b_1 & \\ldots & b_{n} \\end{bmatrix}^{T},$$\n",
      "\n",
      "we have that\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{bmatrix} g_{1}(x) \\\\ g_{2}(x) \\\\ \\vdots \\\\ g_{n}(x) \\end{bmatrix} =  f_{\\rm a}.(W x + b).\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Thus, we can express the output of this neural network as\n",
      "\n",
      "\\begin{equation}\n",
      "g(x) = \\textrm{sum}\\left( f_{\\rm a}.(W x + b) \\right),\n",
      "\\end{equation}\n",
      "\n",
      "where the operator $\\textrm{sum}(\\cdot)$ adds up the elements of the vector."
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us rewrite this expression via a matrix vector product and the \"dot\" operator.\n",
    "\n",
    "Define the $n \\times d$ matrix\n",
    "\n",
    "$$W = \\begin{bmatrix} w_{11} & \\ldots & w_{1d}  \\\\\n",
    "                      w_{21} & \\ldots & w_{2d} \\\\\n",
    "                      \\vdots & \\vdots & \\vdots \\\\\n",
    "                      w_{n1} & \\ldots & w_{nd} \\end{bmatrix}.$$\n",
    "                      \n",
    "Then, for the vector of biases\n",
    "\n",
    "$$ b = \\begin{bmatrix} b_1 & \\ldots & b_{n} \\end{bmatrix}^{T},$$\n",
    "\n",
    "we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix} g_{1}(x) \\\\ g_{2}(x) \\\\ \\vdots \\\\ g_{n}(x) \\end{bmatrix} =  f_{\\rm a}.(W x + b).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus, we can express the output of this neural network as\n",
    "\n",
    "\\begin{equation}\n",
    "g(x) = \\textrm{sum}\\left( f_{\\rm a}.(W x + b) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where the operator $\\textrm{sum}(\\cdot)$ adds up the elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jbook": {
     "section_id": "learnmany",
     "class": "MCCell",
     "editable": true,
     "exercise_id": "sumg",
     "mc_answers": [
      {
       "text": "`sum(f_a.(W*X.+b))` -- the `.+b` is equivalent to `b*ones(1,size(W*X,2))`",
       "uuid": "d5b4e316-787d-4e7d-94e5-05ca41cfebc5"
      },
      {
       "text": "`sum(f_a.(W*X.+b),2)` ",
       "uuid": "acf3c47f-d69e-4487-9040-6e73743d0f0b"
      },
      {
       "text": "`sum(f_a.(W*X.+b),1)` ",
       "uuid": "0e5607f2-1a7b-471d-b1c2-1beb72d34b6b"
      }
     ],
     "mc_prompt": "We would like to pass as input an $d \\times N$ matrix $X$. Suppose $W$ is an $n \\times d$ matrix and $b$ is an $n$ dimensional vector. Which of the following commands returns the $1 \\times N$ matrix of  outputs?",
     "current_ui_view": "student"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ]
}